# Large-Language-Models
Repository of all work related to Large Language Models Construction and Training

## Projects:-
### GPT Bigram model Trained on Tiny Shakespeare 
GPT encoder model trained on ~300K character based tokens.

- [Directory](GPT(tiny_shakespeare))
- [Documentation](GPT(tiny_shakespeare)/README.md)

### Tokenizer(GPT2)
Tokenizer based on BPE and regex tokenization similar to that of the GPT series from OpenAI.

- [Directory](Tokenizer)
- [Documentation](Tokenizer/README.md)

### GPT2 Model


